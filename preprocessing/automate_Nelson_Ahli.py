# -*- coding: utf-8 -*-
"""automate_Nelson-Ahli

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ceOomnlzCQP6Cqk39zRp_oivIPK4Zw8M

# **1. Perkenalan Dataset**

Tahap pertama, Anda harus mencari dan menggunakan dataset dengan ketentuan sebagai berikut:

1. **Sumber Dataset**:  Kaggle
2. **Judul**: Telco Customer Churn
3. **Deskripsi**: Predict behavior to retain customers. You can analyze all relevant customer data and develop focused customer retention programs.

# **2. Import Library**

Pada tahap ini, Anda perlu mengimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model machine learning atau deep learning.
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, OrdinalEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split

from google.colab import files
from joblib import dump
import os

"""# **3. Memuat Dataset**

Pada tahap ini, Anda perlu memuat dataset ke dalam notebook. Jika dataset dalam format CSV, Anda bisa menggunakan pustaka pandas untuk membacanya. Pastikan untuk mengecek beberapa baris awal dataset untuk memahami strukturnya dan memastikan data telah dimuat dengan benar.

Jika dataset berada di Google Drive, pastikan Anda menghubungkan Google Drive ke Colab terlebih dahulu. Setelah dataset berhasil dimuat, langkah berikutnya adalah memeriksa kesesuaian data dan siap untuk dianalisis lebih lanjut.

Jika dataset berupa unstructured data, silakan sesuaikan dengan format seperti kelas Machine Learning Pengembangan atau Machine Learning Terapan
"""

files.upload()
print("Success")

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d blastchar/telco-customer-churn
!unzip telco-customer-churn.zip

dataset = "WA_Fn-UseC_-Telco-Customer-Churn.csv"
df = pd.read_csv(dataset)

df

"""# **4. Exploratory Data Analysis (EDA)**

Pada tahap ini, Anda akan melakukan **Exploratory Data Analysis (EDA)** untuk memahami karakteristik dataset.

Tujuan dari EDA adalah untuk memperoleh wawasan awal yang mendalam mengenai data dan menentukan langkah selanjutnya dalam analisis atau pemodelan.
"""

df.head()

df.info()

df.describe(include="all")

df_fixed = df.copy()
df_fixed = df_fixed.drop(['customerID'], axis=1)
df_fixed['TotalCharges'] = pd.to_numeric(df_fixed['TotalCharges'], errors='coerce')

print("Drop kolom customerID dan Perbaikan tipe data kolom TotalCharges")

le = LabelEncoder()
df_lencoder = df_fixed.copy()

df_lencoder['Churn'] = le.fit_transform(df_lencoder['Churn'])

df_onehot = df_lencoder.copy()
df_onehot = pd.get_dummies(df_onehot, dtype=int)

df_onehot.head()

plt.figure(figsize=(12, 10))
correlation_matrix = df_onehot.corr()

sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Matrix')
plt.show()

target_corr = df_onehot.corr()['Churn']

target_corr_sorted = target_corr.abs().sort_values(ascending=False)

plt.figure(figsize=(10, 6))
target_corr_sorted.plot(kind='bar')
plt.title(f'Correlation with Churn')
plt.xlabel('Variables')
plt.ylabel('Correlation Coefficient')
plt.show()

num_vars = df_onehot.shape[1]

n_cols = 4
n_rows = -(-num_vars // n_cols)

fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 4))

axes = axes.flatten()

for i, column in enumerate(df_onehot.columns):
    df_onehot[column].hist(ax=axes[i], bins=20, edgecolor='black')
    axes[i].set_title(column)
    axes[i].set_xlabel('Value')
    axes[i].set_ylabel('Frequency')

for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""# **5. Data Preprocessing**

Pada tahap ini, data preprocessing adalah langkah penting untuk memastikan kualitas data sebelum digunakan dalam model machine learning.

Jika Anda menggunakan data teks, data mentah sering kali mengandung nilai kosong, duplikasi, atau rentang nilai yang tidak konsisten, yang dapat memengaruhi kinerja model. Oleh karena itu, proses ini bertujuan untuk membersihkan dan mempersiapkan data agar analisis berjalan optimal.

Berikut adalah tahapan-tahapan yang bisa dilakukan, tetapi **tidak terbatas** pada:
1. Menghapus atau Menangani Data Kosong (Missing Values)
2. Menghapus Data Duplikat
3. Normalisasi atau Standarisasi Fitur
4. Deteksi dan Penanganan Outlier
5. Encoding Data Kategorikal
6. Binning (Pengelompokan Data)

Cukup sesuaikan dengan karakteristik data yang kamu gunakan yah. Khususnya ketika kami menggunakan data tidak terstruktur.
"""

def preprocess_data(data, target_column, save_path, file_path, final_dataset_path):
    data = data.copy()
    data = data.drop_duplicates()

    data = data.drop(['customerID'], axis=1)
    data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')

    numeric_features = ['tenure', 'MonthlyCharges', 'TotalCharges']
    categorical_features = data.select_dtypes(include=['object']).columns.tolist()
    ordinal_features = ['Churn', 'tenure_binning']

    column_names = data.columns
    column_names = data.columns.drop(target_column)

    df_header = pd.DataFrame(columns=column_names)

    df_header.to_csv(file_path, index=False)
    print(f"Nama kolom berhasil disimpan ke: {file_path}")

    if target_column in numeric_features:
        numeric_features.remove(target_column)
    if target_column in categorical_features:
        categorical_features.remove(target_column)

    data[numeric_features] = data[numeric_features].fillna(data[numeric_features].median())

    binnings = [0, 12, 24, 48, 60, 80]
    labels = ['< 1 Year', '1-2 Years', '2-4 Years', '4-5 Years', '> 5 Years']

    data['tenure_binning'] = pd.cut(data['tenure'], bins=binnings, labels=labels, include_lowest=True)

    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])

    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ('encoder', OneHotEncoder(handle_unknown='ignore'))
    ])

    ordinal_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OrdinalEncoder(categories=[labels]))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features),
            ('ord', ordinal_transformer, ordinal_features)
        ]
    )

    data = pd.DataFrame(preprocessor.fit_transform(data), columns=preprocessor.get_feature_names_out())
    data.to_csv(file_path, index=False)
    print(f"Data berhasil disimpan ke: {file_path}")

    X = data.drop(columns=[target_column])
    y = data[target_column]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    X_train = preprocessor.fit_transform(X_train)
    X_test = preprocessor.transform(X_test)

    dump(preprocessor, save_path)

    return X_train, X_test, y_train, y_test

target_column = 'Churn'
save_path = 'preprocessing/preprocessor_pipeline.joblib'
file_path = 'preprocessing/data.csv'
final_dataset_path = 'preprocessing/WA_Fn-UseC_-Telco-Customer-Churn_preprocessing.csv'

if __name__ == "__main__":
    csv_file = "WA_Fn-UseC_-Telco-Customer-Churn.csv"

    df = pd.read_csv(csv_file)

    X_train, X_test, y_train, y_test = preprocess_data(df, target_column, save_path, file_path, final_dataset_path)

    print("Preprocess Done")